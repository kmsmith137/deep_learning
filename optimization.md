### Specialized references for optimization, training

  - Useful blog posts on Nesterov acceleration: [1](https://blogs.princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent/) [2](https://blogs.princeton.edu/imabandit/2014/03/06/nesterovs-accelerated-gradient-descent-for-smooth-and-strongly-convex-optimization/) | [3](https://blogs.princeton.edu/imabandit/2015/06/30/revisiting-nesterovs-acceleration/)

  - [Weight normalization](https://arxiv.org/abs/1602.07868)

  - [Equilibriated adaptive learning rates](https://arxiv.org/abs/1502.04390)

### Adaptive learning rates

  - [No more pesky learning rates](https://arxiv.org/pdf/1206.1106.pdf)

  - [Faster gradient descent via an adaptive learning rate](http://www.cs.toronto.edu/~mravox/p4.pdf)

  - [Learning rate adaptation in stochastic gradient descent](http://www.dcs.bbk.ac.uk/~gmagoulas/c-7.pdf)
